{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ncV0PbNhd-n",
        "outputId": "5a0acdef-80d3-449d-e8bc-04696fc74628"
      },
      "source": [
        "# Downloading necessary files\n",
        "! wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "! gzip -d \"GoogleNews-vectors-negative300.bin.gz\"\n",
        "! wget -c \"https://raw.githubusercontent.com/Prasanth-s-n/NLP_Python/master/capitals.txt\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-06 16:18:07--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.197.192\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.197.192|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G   100MB/s    in 16s     \n",
            "\n",
            "2021-12-06 16:18:24 (95.8 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n",
            "--2021-12-06 16:19:44--  https://raw.githubusercontent.com/Prasanth-s-n/NLP_Python/master/capitals.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 154922 (151K) [text/plain]\n",
            "Saving to: ‘capitals.txt’\n",
            "\n",
            "capitals.txt        100%[===================>] 151.29K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2021-12-06 16:19:47 (56.5 MB/s) - ‘capitals.txt’ saved [154922/154922]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G0Ur6lziYKT"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "#install if you dont have already\n",
        "#!pip install gensim\n",
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPfIBLqiiZ5T"
      },
      "source": [
        "def get_vectors(embeddings,words):\n",
        "    X = np.zeros((1, 300))\n",
        "    for word in words:\n",
        "        english = word\n",
        "        eng_emb = embeddings[english]\n",
        "        X = np.row_stack((X, eng_emb))\n",
        "    X = X[1:,:]\n",
        "    return X"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-c-IXiFibre"
      },
      "source": [
        "def get_word_embeddings(embeddings,set_words,complete=False):\n",
        "\n",
        "    word_embeddings = {}\n",
        "    for word in embeddings.vocab:\n",
        "        if word in set_words:\n",
        "            word_embeddings[word] = embeddings[word]\n",
        "        if complete:\n",
        "            word_embeddings[word] = embeddings[word]\n",
        "    return word_embeddings\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8k5vtDKieCU"
      },
      "source": [
        "def cos_similarity(W1,W2):\n",
        "    num = np.dot(W1,W2)\n",
        "    det = np.linalg.norm(W1)*np.linalg.norm(W2)\n",
        "    return (num/det)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvS0aqFBif14"
      },
      "source": [
        "def euclidean(W1,W2):\n",
        "    return np.linalg.norm(np.subtract(W1,W2))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwmUK0QWihhj"
      },
      "source": [
        "# we can use this to find 4th word related to 3rd word similar to how 1 and 2 related.\n",
        "#for now since my subset mostly hav countries and cities, i have named this function as predict country\n",
        "def predict_country(city1, country1, city2, embeddings):\n",
        "    \n",
        "    group = set((city1, country1, city2))\n",
        "\n",
        "    city1_emb = embeddings[city1]\n",
        "    country1_emb = embeddings[country1]\n",
        "    city2_emb = embeddings[city2]\n",
        "    \n",
        "    vec = city2_emb-city1_emb+country1_emb\n",
        "\n",
        "    # Initialize the similarity to -1 (it will be replaced by a similarities that are closer to +1)\n",
        "    similarity = -1\n",
        "    \n",
        "    # initialize country to an empty string\n",
        "    country = ''\n",
        "    \n",
        "    # loop through all words in the embeddings dictionary\n",
        "    for word in embeddings.keys():\n",
        "        if word not in group:\n",
        "\n",
        "            word_emb = embeddings[word]\n",
        "            cur_similarity = cos_similarity(vec,word_emb) #current similarity\n",
        "            #checking whether the similarity is greater than previous similarity\n",
        "            if cur_similarity > similarity:\n",
        "                similarity = cur_similarity\n",
        "                country = word\n",
        "    return country,similarity"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stiH74d5ijM1"
      },
      "source": [
        "#install if you dont have already\n",
        "#!pip install gensim\n",
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrALRwkWik7x"
      },
      "source": [
        "#this will take around 2-3mins\n",
        "embeddings = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNUi2yt5imn_",
        "outputId": "b7e72ced-bafd-4d1d-94bf-126e39032587"
      },
      "source": [
        "f = open('capitals.txt', 'r').read()\n",
        "nltk.download('punkt')\n",
        "set_words = set(nltk.word_tokenize(f))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhhYobLnioCr"
      },
      "source": [
        "word_embeddings=get_word_embeddings(embeddings,set_words)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwFmtd11ip5a",
        "outputId": "30590bd8-b569-4a75-d416-ada9a10b86f0"
      },
      "source": [
        "city1='Moscow'\n",
        "country1='Russia'\n",
        "city2='NewDelhi'\n",
        "\n",
        "# Predicting country from city\n",
        "country2,similarity=predict_country(city1,country1,city2,word_embeddings)\n",
        "print(\"Country is {} with cosine similarity being {}\".format(country2,similarity))\n",
        "\n",
        "\n",
        "# Predicting city from country\n",
        "country3='England'\n",
        "city3,similarity = predict_country(country1,city1,country3,word_embeddings)\n",
        "print(\"City is {} with cosine similarity being {}\".format(city3,similarity))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Country is India with cosine similarity being 0.5113393664360046\n",
            "City is London with cosine similarity being 0.6056334972381592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLbVHanqirIi"
      },
      "source": [
        "def get_accuracy(word_embeddings, data):\n",
        "    \n",
        "    num_correct = 0\n",
        "    for i, row in data.iterrows():\n",
        "        city1 = row[0]\n",
        "        country1 = row[1]\n",
        "        city2 =  row[2]\n",
        "        country2 = row[3]\n",
        "        predicted_country2, _ = predict_country(city1,country1,city2,word_embeddings)\n",
        "\n",
        "        if predicted_country2 == country2:\n",
        "            num_correct += 1\n",
        "\n",
        "    m = len(data)\n",
        "    accuracy = num_correct/m\n",
        "    return accuracy\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qFq8tD0isiA"
      },
      "source": [
        "data = pd.read_csv('capitals.txt', delimiter=' ')\n",
        "data.columns = ['city1', 'country1', 'city2', 'country2']\n",
        "acc=get_accuracy(word_embeddings,data)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qwog3e_Biu1R",
        "outputId": "36ddfe31-cc51-43c2-b38f-3b8a19cc6da1"
      },
      "source": [
        "#print({acc:.2f})\n",
        "print(\"Accuracy is {:.2f}\".format(acc))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is 0.92\n"
          ]
        }
      ]
    }
  ]
}